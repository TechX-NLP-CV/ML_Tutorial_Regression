{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Machine Learning Tutorial: Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Regression?\n",
    "Regression analysis is the process to find the relationship between the ***dependent variable*** and the ***independent variable***.\n",
    "\n",
    "Generally, regression analysis is mainly used for 2 completely different purposes:\n",
    "\n",
    "1. ***Predict*** or ***estimate*** the future value/trend using the observed data. This is widely used in salary forecasting, price estimation, drug efficiency test and so on.\n",
    "\n",
    "2. ***Reveal the causal relationships*** between the dependent and the independt variable (How one variable could influence the other).\n",
    "\n",
    "Within the first condition, we have to carefully ***justify why the prediction using regression is valid and accurate***.\n",
    "\n",
    "Within the second condition, we have to ***explain where this causal relationship comes from***.\n",
    "\n",
    "There are lots of types of regression in the machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Linear Regression and Multiple Linear Regression\n",
    "As we mentioned before, the relationship within the linear regression model is strictly linear.\n",
    "\n",
    "The difference between the linear regresssion and the multiple inear regression is that:\n",
    "\n",
    "1. Within linear regression, the input x is a scalar number and the output y is also a scalar number.\n",
    "\n",
    "2. Within multiple linear regression, the input x has multiple features (must be greater than 1) and the output y is a scalar number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Logistic Regression\n",
    "The logistic regression use the sigmoid function $f(x)=\\frac{1}{1+e^{-x}}$ to output a value between 0~1.\n",
    "\n",
    "This regression model could be used to solve classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Polynomial Regression\n",
    "The polynomial regression model could learn a non-linear relationship $y=a_0+a_1x+a_2x^2...$\n",
    "\n",
    "From the above equation we could see that, ***the original feature is transformed to polynomial features of given degrees and then the final relationship is modeled as a linear model.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Support Vector Regression (SVR)\n",
    "This is very similar to the Support Vector Machine (which we covered in the classification tutorial) and slight modified to solve regression problems.\n",
    "\n",
    "The core goal of SVR is that we want to have the maximum number of data points between the boundary lines and the best-fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Decision Tree Regression\n",
    "This regression model use a tree-like structure to solve the problem.\n",
    "\n",
    "Basically you will meet lots of \"test\" stored in the roots. You could choose branches based on the answer for each \"test\" and reach the leaf node which embeds the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Ridge Regression (L2 Regularisation)\n",
    "This is the more powerful and robust version of linear regression by adding the L2 regularisation term. We will return to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Lasso Regression (L1 Regularisation)\n",
    "This is similar to the ridge regression but using L1 regularisation term instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is Supervised Learning?\n",
    "Supervised Learning is the machine learning task to learn a mapping between the input features and the output and the goal is to ***generalise from the training data to accurately find the result for unseen data***.\n",
    "\n",
    "Here, we will start with the linear regression to help you understand more about the supervised learning and feel the power of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Training, Validation and Test Sets (Very Important!!!)\n",
    "As we have seen, the reason why machine learning algorithms could have this fancy performance is that generally they use a very large amount of data to \"learn\" how to sovle the problem.\n",
    "\n",
    "Within this \"learning\" process, we could divide data into training, validation and test datasets for different purpose:\n",
    "\n",
    "1. Training set ***(could \"see\", could \"use\" in the training)***: The model could access to those dataset to optimize the weights and do calculations in the training.\n",
    "\n",
    "2. Validation set ***(could \"see\", can't \"use\" in the training)***: During the training, the model could only use the validation set to determine how good it could perform.\n",
    "\n",
    "3. Test set ***(can't \"see\", can't \"use\" in the training)***: After training, the model could use the test set to determine how good it could perform on the unseen data.\n",
    "\n",
    "The validation set and test set are very similar as they are both used for evaluating the accuracy of the model. However, the key difference is that (very important!!!):\n",
    "\n",
    "1. ***Validation set could be used during the training process, which means you could observe the loss information and tune the hyperparameters but can't use it for optimisation and calculation.***\n",
    "\n",
    "2. ***Test set could only be used after the training is finished. It is used to check how the model could perform on absolutely unseen data. In other words, you can't do any modification to the model once you use the test set.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Loss Function\n",
    "Loss is defined as the difference between the predicted result and the true value, which could be used to measure the distance between points in the feature spaces.\n",
    "\n",
    "A valid loss function must obey the following rules:\n",
    "\n",
    "1. The result is non-negative.\n",
    "\n",
    "2. The loss/distance is symmetric: $Loss(A, B)=Loss(B, A)$\n",
    "\n",
    "3. Triangular Inequality: $Loss(A,C) \\leqslant Loss(A,B)+Loss(B,C)$ for any possible A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Square Loss\n",
    "Within this tutorial, we will simple use the square loss as our loss function: $Loss(x,y)=(x-y)^2$.\n",
    "\n",
    "There are lots of loss function choices, but one reason why we use square loss is that: ***The higher the difference is, the much more penalty would produce.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Error\n",
    "Error is the deviation between the observed result and the ideal truth value.\n",
    "\n",
    "Within the regression model, there are 2 main types of errors:\n",
    "\n",
    "1. Structual Erorr: This error raises when the chosen model is not complex enough for the dataset (use linear regression model to predict a polynomial trend).\n",
    "\n",
    "2. Estimation Error: This error raises when the training dataset is not sufficient (can't find the right line expression even through the trend is linear).\n",
    "\n",
    "The structual error could be solved by ***plotting the data if possible to visually determine the trend and the degree of the relationship.***\n",
    "\n",
    "The estimation error could be sovled by ***collecting more data to enlarge the training set***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Empirical Risk\n",
    "Although we could train our model to work well on our training/validation sets, we absolutely have no idea how it could perform on the real unseen data.\n",
    "\n",
    "To tackle this problem, we have to use the principle of empirical risk minimisation here.\n",
    "\n",
    "Empirical risk could estimate how the model might perform on the unseen data using the accuracy on the observed dataset.\n",
    "\n",
    "Empirical risk is mathematically defined as: $R(model)=\\frac{1}{N}\\sum_{i=1}^{N} Loss(y_i , model(x_i))$\n",
    "\n",
    "This means that our most expected model should be able to minimize the empirical risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Gradient Descent: Concept (Very Important!)\n",
    "Gradient descent is a very efficient and powerful method used during the optimisation in the machine learning algorithm.\n",
    "\n",
    "The goal of optimisation is ***to find the local minimum (global minimum if you are super lucky) of the loss function to get a more accurate model***.\n",
    "\n",
    "Imagine you are on a mountain and you want to go back to the bottom of the mountain. Here is a list of what you might do:\n",
    "\n",
    "1. Find the path pointing downwards, which is the inverse way of the path you climbed up ***(Find the negative of the gradient of the loss function at the certain point).***\n",
    "\n",
    "2. Go down towards the buttom by one step ***(Update the loss by using multiplying the negative gradient and the learning rate).***\n",
    "\n",
    "3. Repeat step 1 and 2 until you reach the buttom ***(For each epoch, find the new gradient and update the loss using that).***\n",
    "\n",
    "We will explain this more in the gradient-based method part later.\n",
    "\n",
    "![Gradient Descent](https://miro.medium.com/max/1400/1*G1v2WBigWmNzoMuKOYQV_g.png \"Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Closed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the local minimum occurs when the first-derivative is equal to 0 in a convex function.\n",
    "\n",
    "This means that we could directly solve the optmized solution using this principle for linear regression and multiple linear regression.\n",
    "\n",
    "Our following part is under the setting of multiple linear regression, we encourage you to analyse the linear regression model first, which is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.1 Problem Definition\n",
    "\n",
    "Within multiple linear regression, we want to use $(x_1, x_2, ... x_N)$ and $(y_1, y_2, ... y_N)$ to learn the mapping: $y=\\theta \\cdot x$ where $x$ is a D-dimensional vector and $y$ is a scalar.\n",
    "\n",
    "Here we use $\\theta$ to represent the weight of the model so $y=\\theta \\cdot x$ is equivalent to: $y=model(x)$\n",
    "\n",
    "Recalling from the previous part, the empirical risk is: $R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N} Loss(y_i-\\theta \\cdot x_i)$\n",
    "\n",
    "To simplify the calculation, we will use the 0.5*Square Loss as our loss function (You will feel the power soon).\n",
    "\n",
    "Combining all information, our final objective function is: $R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{2}(y_i-\\theta \\cdot x_i)^2$\n",
    "\n",
    "The above equation is exactly equivalent to 0.5*Mean Square Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.2 Detailed Calculation\n",
    "Taking the first derivative with respect to $\\theta$, we could get: $\\nabla _\\theta R(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N} (y_i-\\theta \\cdot x_i)x_i$\n",
    "\n",
    "Simplify the expression we could get: $\\nabla _\\theta R(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N} y_ix_i + \\frac{1}{N}\\sum_{i=1}^{N} \\theta \\cdot x_ix_i$\n",
    "\n",
    "he term $\\frac{1}{N}\\sum_{i=1}^{N} \\theta \\cdot x_ix_i$ could be re-written as $\\frac{1}{N}\\sum_{i=1}^{N} x_i\\theta \\cdot x_i = \\frac{1}{N}\\sum_{i=1}^{N} x_i(x_i)^T\\theta$ \n",
    "\n",
    "If we let $A=\\frac{1}{N}\\sum_{i=1}^{N} x_i(x_i)^T$ and $b=-\\frac{1}{N}\\sum_{i=1}^{N} y_ix_i$, the above equation could be simply written as: $\\nabla _\\theta R(\\theta)=-b+A\\theta$\n",
    "\n",
    "Our final solution is: $\\theta=A^{-1}b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.3 Detaild Calculation in Matrix Form\n",
    "To process a large amout of data with lots of features, we have to store them as a big matrix.\n",
    "\n",
    "Within the multiple linear regression, we store:\n",
    "\n",
    "1. N different y values as a (N,1) matrix $Y$\n",
    "\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}$$\n",
    "\n",
    "2. N different x values with d features as a (N,d+1) matrix $X$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & x_{1,1} & x{1,2} & \\dots & x_{1,d} \\\\\n",
    "1 & x_{2,1} & x{2,2} & \\dots & x_{2,d} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
    "1 & x_{N,1} & x{N,2} & \\dots & x_{N,d} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "3. $\\theta$ as (d+1, 1) matrix\n",
    "\n",
    "$$\\begin{bmatrix} \\theta _0 \\\\ \\theta _1 \\\\ \\vdots \\\\ \\theta _N \\end{bmatrix}$$\n",
    "\n",
    "In this way, we could have $\\theta X=\\theta _0+\\sum_{i=1}^{d} \\theta _ix_i$\n",
    "\n",
    "Our empirical risk function could be expressed as $R(\\theta)=\\frac{1}{N}(Y-\\theta X)^T(Y-\\theta X)=\\frac{1}{N}(Y^TY-Y^TX\\theta-\\theta ^TX^TY+\\theta ^TX^TX\\theta)$\n",
    "\n",
    "Notice that $Y^TX\\theta$ is a scalar, for any scalar $r$ we have $r^T=r$\n",
    "\n",
    "So $Y^TX\\theta=(Y^TX\\theta)^T=\\theta ^TX^TY$\n",
    "\n",
    "So $R(\\theta)=\\frac{1}{N}(Y^TY-2\\theta ^TX^TY+\\theta ^TX^TX\\theta)$\n",
    "\n",
    "Taking the derivative with respect to $\\theta$, we could get $\\nabla _\\theta R(\\theta)=-2X^TY+2X^TX\\theta$\n",
    "\n",
    "Solve the above equation we could get: $\\theta=(X^TX)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.4 Further Analysis\n",
    "No worry if you can't understand all detailed calculations above. What you need to know is that we could find the solution directly by solving the differential equation.\n",
    "\n",
    "But can we always use this method? The answer is NO.\n",
    "\n",
    "Recalling back from the section 3.6.2, we have the final solution $\\theta=A^{-1}b$\n",
    "\n",
    "This solution is only valid if we could find the inverse matrix of $A$\n",
    "\n",
    "This means that ***If we don't have enough training set (which could decrease the rank of A), we still can't find the closed solution.***\n",
    "\n",
    "Besides this, ***the computational cost for matrix calculation is very high so it is definitely not a good idea to find the closed solution for a large amount of data.***\n",
    "\n",
    "To tackle those problems, we could use gradient descent based method instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Gradient Descent Based Method\n",
    "We could use gradient descent based method instead as it could save a lot of time while keeping a relatively high accuracy.\n",
    "\n",
    "The gradient descent based method contains the following main steps:\n",
    "\n",
    "1. Randomly choose t points from the total N points as the training set and the rest could form the validation set.\n",
    "\n",
    "2. Calculate the training loss using the current model and training set.\n",
    "\n",
    "3. Update the model according to the current gradient information. \n",
    "\n",
    "4. Calculate the validation loss using the updated model and validation set.\n",
    "\n",
    "5. Repeat step 1 to 4 until reaching the maximum number of epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.7.1 Detailed Gradient Descent Method: Stochastic Gradient Descent (SGD)\n",
    "The general equation to optimize the model is: $\\theta=\\theta - rate\\cdot\\nabla _\\theta R(\\theta)$\n",
    "\n",
    "Theoretically, we need to use the entire dataset to compute gradient accurately and optimize the model. However, this could be painful if we have lots of data.\n",
    "\n",
    "Alternatively, we could estimate the gradient by calculating the accurate gradient at a specific chosen point, which is known as stochastic gradient descent (SGD).\n",
    "\n",
    "This method could significantly decrease the computational cost and it makes lots of machine learning and deep learning algrotihms powerful.\n",
    "\n",
    "Our objective function (empirical risk) is: $R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{2}(y_i-\\theta \\cdot x_i)^2$\n",
    "\n",
    "Taking the first derivative with respect to $\\theta$: $\\nabla _\\theta R(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N} (y_i-\\theta \\cdot x_i)x_i = \\frac{1}{N}\\sum_{i=1}^{N} (\\theta \\cdot x_i-y_i)x_i$\n",
    "\n",
    "Taking the learning rate into account: $\\theta _{new}=\\theta -rate\\cdot\\frac{1}{N}\\sum_{i=1}^{N} (\\theta \\cdot x_i-y_i)x_i$\n",
    "\n",
    "Because we are only interested in one specfic point, we could simplify it into: $\\theta _{new}=\\theta -rate\\cdot(\\theta \\cdot x_i-y_i)x_i$ where $i$ is randomly chosen from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Regularisation\n",
    "Regularisation could be considered as a resistance for model to perfectly fit the training data.\n",
    "\n",
    "If our model could perfectly fit the training data (very low training loss), this means that it could also fit the noise within the training set very well and generally can't predict the result for unseen data.\n",
    "\n",
    "By adding the regularisation term, only a very big change could update the model, which makes it more robust and stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.1 Gradient Descent Based Method with Regularisation (L2)\n",
    "Within this tutorial, we will focus on the L2 Regularisation.\n",
    "\n",
    "The new objective function becomes: $J_ {\\lambda, N}(\\theta)=\\frac{\\lambda}{2} \\lVert \\theta \\rVert_2^2 + R(\\theta)=\\frac{\\lambda}{2}\\sum_{i=0}^N \\theta _i ^2 + R(\\theta)$\n",
    "\n",
    "Similarly, by taking the first derivative with respect to $\\theta$ and only interested in one point, we could get: $\\theta _{new}=\\theta -rate\\cdot(\\lambda\\theta+\\theta\\cdot x_i-y_i)x_i$ where $i$ is randomly chosen from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear Regression from Scratch\n",
    "For this tutorial, we are going to solve the Boston House Price Estimation problem step by step.\n",
    "\n",
    "We will directly start implementing the multiple linear regression algorithm as it could be also applied to linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Dataset Loading\n",
    "The whole dataset contains 506 houses with 13 different features:\n",
    "\n",
    "1. CRIM: the per capita crime rate of the town\n",
    "\n",
    "2. ZN: the proportion of residential land zoned for lots over 25,000 sq.ft\n",
    "\n",
    "3. INDUS: the proportion of non-retail nusiness acres per town\n",
    "\n",
    "4. CHAS: Charles River dummy variable (1 if tract bounds river, 0 otherwise)\n",
    "\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "6. RM: average number of rooms per dwelling\n",
    "\n",
    "7. AGE: the proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "8. DIS: weighted distances to five Boston emplyment centers\n",
    "\n",
    "9. RAD: index of accessibility to radial highways\n",
    "\n",
    "10. TAX: full-value property-tax rate per $10,000\n",
    "\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "\n",
    "12. B: 1000(Bk — 0.63)², where Bk is the proportion of [people of African American descent] by town\n",
    "\n",
    "13. Percentage of lower status of the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 506 houses within the dataset and each house has 13 features.\n",
      "There are also 506 corresponding prices.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "def loadBoston():\n",
    "    '''\n",
    "        Load the boston house price dataset and return points\n",
    "    and labels separately\n",
    "\n",
    "    Return:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "    '''\n",
    "    # load_boston() could direcly load the dataset into the required form\n",
    "    # For more information, please check: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "    dataset, label=load_boston(return_X_y=True)\n",
    "    return dataset, label\n",
    "\n",
    "dataset, label=loadBoston()\n",
    "print(f\"There are {dataset.shape[0]} houses within the dataset and each house has {dataset.shape[1]} features.\")\n",
    "print(f\"There are also {label.shape[0]} corresponding prices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we need to insert a column of \"1\" at the beginning of dataset to change the shape into (N, d+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After modification, there are 506 houses now and each house has 14 features.\n",
      "The first feature value of the house is: 1.0\n"
     ]
    }
   ],
   "source": [
    "def addColumn(dataset):\n",
    "    '''\n",
    "        Add a column of \"1\" to the beginning of the dataset\n",
    "    to change the shape into (N, d+1)\n",
    "\n",
    "    Argument:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, dataset with an extra column, (the number of houses, the number of features for each house+1)\n",
    "    '''\n",
    "    # np.ones() could be used to construct an array full of 1 with the given shape\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.ones.html\n",
    "    column=np.ones((1, dataset.shape[0]))\n",
    "    \n",
    "    # np.insert() could be used to insert the column at the specific position\n",
    "    # obj=0 means we want to insert at the index 0\n",
    "    # axis=1 means we want to insert a column\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.insert.html?highlight=insert#numpy.insert\n",
    "    return np.insert(dataset, 0, column, axis=1)\n",
    "\n",
    "modifiedDataset=addColumn(dataset)\n",
    "print(f\"After modification, there are {modifiedDataset.shape[0]} houses now and each house has {modifiedDataset.shape[1]} features.\")\n",
    "print(f\"The first feature value of the house is: {modifiedDataset[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 $\\theta$  Initialisation\n",
    "For the gradient descent based method, we need to have an initial $\\theta$ matrix, which is full of zero with shape (d+1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the initial theta is: (14, 1)\n"
     ]
    }
   ],
   "source": [
    "def initialiseTheta(dataset):\n",
    "    '''\n",
    "        Initialise and return the theta matrix according to the shape of the dataset\n",
    "    \n",
    "    Argument:\n",
    "        dataset: np.ndarray, dataset with an extra column, (the number of houses, the number of features for each house+1)\n",
    "    \n",
    "    Return:\n",
    "        theta: np.ndarray, full of 0, (the number of features for each house+1,1)\n",
    "    '''\n",
    "    # np.zeros could be used to construct an array full of 0 with the given shape\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.zeros.html?highlight=zeros#numpy.zeros\n",
    "    return np.zeros((dataset.shape[1], 1))\n",
    "\n",
    "theta=initialiseTheta(modifiedDataset)\n",
    "print(f\"The shape of the initial theta is: {theta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Closed Solution (No Regularisation)\n",
    "As we mentioned in the section 3.6.3, our solution could be directly found by $\\theta=(X^TX)^{-1}X^TY$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the result is: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "def closedSolution(dataset, label):\n",
    "    '''\n",
    "        Return the optimized linear regression model by \n",
    "    substituting into the closed form solution\n",
    "\n",
    "        Report when the inverse matrix can't be found\n",
    "\n",
    "    Argument:\n",
    "        dataset: np.ndarray, dataset with an extra column, (the number of houses, the number of features for each house+1)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "\n",
    "    Return:\n",
    "        result: np.ndarray, the optimized theta for the dataset, (the number of features for each house+1,1)\n",
    "    '''\n",
    "    try:\n",
    "        # np.linalg.inv() could find the inverse matrix of the given input matrix\n",
    "        # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html\n",
    "\n",
    "        # np.matmul() could calculate the normal matrix multiplication\n",
    "        # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\n",
    "\n",
    "        # np.transpose() could find the transpose matrix of the input matrix\n",
    "        # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\n",
    "        firstPart=np.linalg.inv(np.matmul(np.transpose(dataset), dataset))\n",
    "        secondPart=np.matmul(np.transpose(dataset), label)\n",
    "        return np.matmul(firstPart, secondPart)\n",
    "    except LinAlgError:\n",
    "        print(\"Can't find the closed solution due to invalid matrix inversion.\")\n",
    "\n",
    "# np.random.rand() could be used to generate a random array with given shape\n",
    "# For more information, please check: https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n",
    "\n",
    "# Here we test N=3, d+1=4 and we expect a (4,1) result\n",
    "testX=np.random.rand(3,4)\n",
    "testLabel=np.random.rand(3,1)\n",
    "print(f\"The shape of the result is: {closedSolution(testX, testLabel).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Gradient Descent Based Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.1 Find Prediction Using $\\theta$\n",
    "The prediction could be simply calculate by the matrix multiplication between $\\theta$ and $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the prediction is: (5, 1)\n"
     ]
    }
   ],
   "source": [
    "def predict(input, theta):\n",
    "    '''\n",
    "        Return the corresponding result by matrix multiplication\n",
    "    \n",
    "    Argument:\n",
    "        input: np.ndarray, arbitrary number of house information, (arbitrary number, the number of features for each house+1)\n",
    "        theta: np.ndarray, represents the linear relationship within the dataset, (the number of features for each house+1, 1)\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, the predicted prices, (arbitrary number, 1)\n",
    "    '''\n",
    "    # The order of theta and input is very important\n",
    "    return np.matmul(input, theta)\n",
    "\n",
    "testInput=np.random.rand(5,6)\n",
    "testTheta=np.random.rand(6,1)\n",
    "result=predict(testInput, testTheta)\n",
    "print(f\"The shape of the prediction is: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 Mean Square Error (MSE)\n",
    "As mentioned in the section 3.6.1, our empirical risk is equivalent to 0.5*MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE loss between the prediction and the target is: 9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def MSE(prediction, target):\n",
    "    '''\n",
    "        Return the MSE between the prediction and the target.\n",
    "\n",
    "        This function could be used for multiple 1-feature result.\n",
    "    \n",
    "    Argument:\n",
    "        prediction: np.ndarray, prediction made by the model, (the number of houses, 1)\n",
    "        target: np.ndarray, the corresponding truth value, (the number of houses, 1)\n",
    "    '''\n",
    "    # Find the difference vector between the prediction and the target\n",
    "    difference=prediction-target\n",
    "    \n",
    "    # Square the difference vector\n",
    "    # np.squrare() could square every element within the vector\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.square.html\n",
    "    differenceSquare=np.square(difference)\n",
    "\n",
    "    # Find the mean of the squared difference\n",
    "    # np.mean() could find the mean of elements in the given axis\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "    return np.mean(differenceSquare)\n",
    "\n",
    "prediction=np.array([1,2,3])\n",
    "target=np.array([4,5,6])\n",
    "print(f\"The MSE loss between the prediction and the target is: {MSE(prediction, target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.3 Update the $\\theta$ Using Gradient\n",
    "As mentioned in the section 3.7.1, we could optimise the $\\theta$ by using: $\\theta _{new}=\\theta -rate\\cdot(\\theta \\cdot x_i-y_i)x_i$ where $i$ is randomly chosen from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "The original theta has the shape: (10, 1)\n",
      "The upated theta has the shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "def updateTheta(x, y, currentTheta, learningRate):\n",
    "    '''\n",
    "        Update and return the new theta according to the optimisation rule\n",
    "    \n",
    "    Argument:\n",
    "        x: np.ndarray, a random point chosen from the training point, (1, the number of features for each house+1)\n",
    "        y: np.ndarray, the corresponding chosen label from the training point, (1,1)\n",
    "        currentTheta: np.ndarray, the current theta before optimisation, (the number of features for each house+1, 1)\n",
    "        learningRate: float, determine the step size along the gradient direction in the gradient descent\n",
    "\n",
    "    Return:\n",
    "        result: np.ndarray, the updated theta, (the number of features for each house+1, 1)\n",
    "    '''\n",
    "    # np.dot() could be used to calculate the dot product of the 2 inputs\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "    \n",
    "    # The order of x and theta here is very important\n",
    "    gradient=(np.dot(x, currentTheta)-y)*x\n",
    "    \n",
    "    # Be careful: the gradient here should has the shape (1, the number of features for each house+1), so we need to use the transpose to update the theta\n",
    "    return currentTheta-learningRate*np.transpose(gradient)\n",
    "\n",
    "testX=np.random.rand(1, 10)\n",
    "testY=np.random.rand(1, 1)\n",
    "testTheta=np.random.rand(10, 1)\n",
    "learningRate=0.1\n",
    "updatedTheta=updateTheta(testX, testY, testTheta, learningRate)\n",
    "print(f\"The original theta has the shape: {testTheta.shape}\")\n",
    "print(f\"The upated theta has the shape: {updatedTheta.shape}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

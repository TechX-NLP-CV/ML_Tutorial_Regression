{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Machine Learning Tutorial: Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Regression?\n",
    "Regression analysis is the process to find the relationship between the ***dependent variable*** and the ***independent variable***.\n",
    "\n",
    "Generally, regression analysis is mainly used for 2 completely different purposes:\n",
    "\n",
    "1. ***Predict*** or ***estimate*** the future value/trend using the observed data. This is widely used in salary forecasting, price estimation, drug efficiency test and so on.\n",
    "\n",
    "2. ***Reveal the causal relationships*** between the dependent and the independt variable (How one variable could influence the other).\n",
    "\n",
    "Within the first condition, we have to carefully ***justify why the prediction using regression is valid and accurate***.\n",
    "\n",
    "Within the second condition, we have to ***explain where this causal relationship comes from***.\n",
    "\n",
    "There are lots of types of regression in the machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Linear Regression and Multiple Linear Regression\n",
    "As we mentioned before, the relationship within the linear regression model is strictly linear.\n",
    "\n",
    "The difference between the linear regresssion and the multiple inear regression is that:\n",
    "\n",
    "1. Within linear regression, the input x is a scalar number and the output y is also a scalar number.\n",
    "\n",
    "2. Within multiple linear regression, the input x has multiple features (must be greater than 1) and the output y is a scalar number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Logistic Regression\n",
    "The logistic regression use the sigmoid function $f(x)=\\frac{1}{1+e^{-x}}$ to output a value between 0~1.\n",
    "\n",
    "This regression model could be used to solve classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Polynomial Regression\n",
    "The polynomial regression model could learn a non-linear relationship $y=a_0+a_1x+a_2x^2...$\n",
    "\n",
    "From the above equation we could see that, ***the original feature is transformed to polynomial features of given degrees and then the final relationship is modeled as a linear model.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Support Vector Regression (SVR)\n",
    "This is very similar to the Support Vector Machine (which we covered in the classification tutorial) and slight modified to solve regression problems.\n",
    "\n",
    "The core goal of SVR is that we want to have the maximum number of data points between the boundary lines and the best-fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Decision Tree Regression\n",
    "This regression model use a tree-like structure to solve the problem.\n",
    "\n",
    "Basically you will meet lots of \"test\" stored in the roots. You could choose branches based on the answer for each \"test\" and reach the leaf node which embeds the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Ridge Regression (L2 Regularisation)\n",
    "This is the more powerful and robust version of linear regression by adding the L2 regularisation term. We will return to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Lasso Regression (L1 Regularisation)\n",
    "This is similar to the ridge regression but using L1 regularisation term instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is Supervised Learning?\n",
    "Supervised Learning is the machine learning task to learn a mapping between the input features and the output and the goal is to ***generalise from the training data to accurately find the result for unseen data***.\n",
    "\n",
    "Here, we will start with the linear regression to help you understand more about the supervised learning and feel the power of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Training, Validation and Test Sets (Very Important!!!)\n",
    "As we have seen, the reason why machine learning algorithms could have this fancy performance is that generally they use a very large amount of data to \"learn\" how to sovle the problem.\n",
    "\n",
    "Within this \"learning\" process, we could divide data into training, validation and test datasets for different purpose:\n",
    "\n",
    "1. Training set ***(could \"see\", could \"use\" in the training)***: The model could access to those dataset to optimize the weights and do calculations in the training.\n",
    "\n",
    "2. Validation set ***(could \"see\", can't \"use\" in the training)***: During the training, the model could only use the validation set to determine how good it could perform.\n",
    "\n",
    "3. Test set ***(can't \"see\", can't \"use\" in the training)***: After training, the model could use the test set to determine how good it could perform on the unseen data.\n",
    "\n",
    "The validation set and test set are very similar as they are both used for evaluating the accuracy of the model. However, the key difference is that (very important!!!):\n",
    "\n",
    "1. ***Validation set could be used during the training process, which means you could observe the loss information and tune the hyperparameters but can't use it for optimisation and calculation.***\n",
    "\n",
    "2. ***Test set could only be used after the training is finished. It is used to check how the model could perform on absolutely unseen data. In other words, you can't do any modification to the model once you use the test set.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Loss Function\n",
    "Loss is defined as the difference between the predicted result and the true value, which could be used to measure the distance between points in the feature spaces.\n",
    "\n",
    "A valid loss function must obey the following rules:\n",
    "\n",
    "1. The result is non-negative.\n",
    "\n",
    "2. The loss/distance is symmetric: $Loss(A, B)=Loss(B, A)$\n",
    "\n",
    "3. Triangular Inequality: $Loss(A,C) \\leqslant Loss(A,B)+Loss(B,C)$ for any possible A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Square Loss\n",
    "Within this tutorial, we will simple use the square loss as our loss function: $Loss(x,y)=(x-y)^2$.\n",
    "\n",
    "There are lots of loss function choices, but one reason why we use square loss is that: ***The higher the difference is, the much more penalty would produce.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Error\n",
    "Error is the deviation between the observed result and the ideal truth value.\n",
    "\n",
    "Within the regression model, there are 2 main types of errors:\n",
    "\n",
    "1. Structual Erorr: This error raises when the chosen model is not complex enough for the dataset (use linear regression model to predict a polynomial trend).\n",
    "\n",
    "2. Estimation Error: This error raises when the training dataset is not sufficient (can't find the right line expression even through the trend is linear).\n",
    "\n",
    "The structual error could be solved by ***plotting the data if possible to visually determine the trend and the degree of the relationship.***\n",
    "\n",
    "The estimation error could be sovled by ***collecting more data to enlarge the training set***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Empirical Risk\n",
    "Although we could train our model to work well on our training/validation sets, we absolutely have no idea how it could perform on the real unseen data.\n",
    "\n",
    "To tackle this problem, we have to use the principle of empirical risk minimisation here.\n",
    "\n",
    "Empirical risk could estimate how the model might perform on the unseen data using the accuracy on the observed dataset.\n",
    "\n",
    "Empirical risk is mathematically defined as: $R(model)=\\frac{1}{N}\\sum_{i=1}^{N} Loss(y_i , model(x_i))$\n",
    "\n",
    "This means that our most expected model should be able to minimize the empirical risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Gradient Descent: Concept (Very Important!)\n",
    "Gradient descent is a very efficient and powerful method used during the optimisation in the machine learning algorithm.\n",
    "\n",
    "The goal of optimisation is ***to find the local minimum (global minimum if you are super lucky) of the loss function to get a more accurate model***.\n",
    "\n",
    "Imagine you are on a mountain and you want to go back to the bottom of the mountain. Here is a list of what you might do:\n",
    "\n",
    "1. Find the path pointing downwards, which is the inverse way of the path you climbed up ***(Find the negative of the gradient of the loss function at the certain point).***\n",
    "\n",
    "2. Go down towards the buttom by one step ***(Update the loss by using multiplying the negative gradient and the learning rate).***\n",
    "\n",
    "3. Repeat step 1 and 2 until you reach the buttom ***(For each epoch, find the new gradient and update the loss using that).***\n",
    "\n",
    "We will explain this more in the gradient-based method part later.\n",
    "\n",
    "![Gradient Descent](https://miro.medium.com/max/1400/1*G1v2WBigWmNzoMuKOYQV_g.png \"Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Closed Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the local minimum occurs when the first-derivative is equal to 0 in a convex function.\n",
    "\n",
    "This means that we could directly solve the optmized solution using this principle for linear regression and multiple linear regression.\n",
    "\n",
    "Our following part is under the setting of multiple linear regression, we encourage you to analyse the linear regression model first, which is very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.1 Problem Definition\n",
    "\n",
    "Within multiple linear regression, we want to use $(x_1, x_2, ... x_N)$ and $(y_1, y_2, ... y_N)$ to learn the mapping: $y=\\theta \\cdot x$ where $x$ is a D-dimensional vector and $y$ is a scalar.\n",
    "\n",
    "Here we use $\\theta$ to represent the weight of the model so $y=\\theta \\cdot x$ is equivalent to: $y=model(x)$\n",
    "\n",
    "Recalling from the previous part, the empirical risk is: $R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N} Loss(y_i-\\theta \\cdot x_i)$\n",
    "\n",
    "To simplify the calculation, we will use the 0.5*Square Loss as our loss function (You will feel the power soon).\n",
    "\n",
    "Combining all information, our final objective function is: $R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{2}(y_i-\\theta \\cdot x_i)^2$\n",
    "\n",
    "The above equation is exactly equivalent to 0.5*Mean Square Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.2 Detailed Calculation\n",
    "Taking the first derivative with respect to $\\theta$, we could get: $\\nabla _\\theta R(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N} (y_i-\\theta \\cdot x_i)x_i$\n",
    "\n",
    "Simplify the expression we could get: $\\nabla _\\theta R(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N} y_ix_i + \\frac{1}{N}\\sum_{i=1}^{N} \\theta \\cdot x_ix_i$\n",
    "\n",
    "he term $\\frac{1}{N}\\sum_{i=1}^{N} \\theta \\cdot x_ix_i$ could be re-written as $\\frac{1}{N}\\sum_{i=1}^{N} x_i\\theta \\cdot x_i = \\frac{1}{N}\\sum_{i=1}^{N} x_i(x_i)^T\\theta$ \n",
    "\n",
    "If we let $A=\\frac{1}{N}\\sum_{i=1}^{N} x_i(x_i)^T$ and $b=-\\frac{1}{N}\\sum_{i=1}^{N} y_ix_i$, the above equation could be simply written as: $\\nabla _\\theta R(\\theta)=-b+A\\theta$\n",
    "\n",
    "Our final solution is: $\\theta=A^{-1}b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.3 Detaild Calculation in Matrix Form\n",
    "To process a large amout of data with lots of features, we have to store them as a big matrix.\n",
    "\n",
    "Within the multiple linear regression, we store:\n",
    "\n",
    "1. N different y values as a (N,1) matrix $Y$\n",
    "\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}$$\n",
    "\n",
    "2. N different x values with d features as a (N,d+1) matrix $X$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & x_{1,1} & x{1,2} & \\dots & x_{1,d} \\\\\n",
    "1 & x_{2,1} & x{2,2} & \\dots & x_{2,d} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
    "1 & x_{N,1} & x{N,2} & \\dots & x_{N,d} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "3. $\\theta$ as (d+1, 1) matrix\n",
    "\n",
    "$$\\begin{bmatrix} \\theta _0 \\\\ \\theta _1 \\\\ \\vdots \\\\ \\theta _N \\end{bmatrix}$$\n",
    "\n",
    "In this way, we could have $\\theta X=\\theta _0+\\sum_{i=1}^{d} \\theta _ix_i$\n",
    "\n",
    "Our empirical risk function could be expressed as $R(\\theta)=\\frac{1}{N}(Y-\\theta X)^T(Y-\\theta X)=\\frac{1}{N}(Y^TY-Y^TX\\theta-\\theta ^TX^TY+\\theta ^TX^TX\\theta)$\n",
    "\n",
    "Notice that $Y^TX\\theta$ is a scalar, for any scalar $r$ we have $r^T=r$\n",
    "\n",
    "So $Y^TX\\theta=(Y^TX\\theta)^T=\\theta ^TX^TY$\n",
    "\n",
    "So $R(\\theta)=\\frac{1}{N}(Y^TY-2\\theta ^TX^TY+\\theta ^TX^TX\\theta)$\n",
    "\n",
    "Taking the derivative with respect to $\\theta$, we could get $\\nabla _\\theta R(\\theta)=-2X^TY+2X^TX\\theta$\n",
    "\n",
    "Solve the above equation we could get: $\\theta=(X^TX)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.4 Further Analysis\n",
    "No worry if you can't understand all detailed calculations above. What you need to know is that we could find the solution directly by solving the differential equation.\n",
    "\n",
    "But can we always use this method? The answer is NO.\n",
    "\n",
    "Recalling back from the section 3.6.2, we have the final solution $\\theta=A^{-1}b$\n",
    "\n",
    "This solution is only valid if we could find the inverse matrix of $A$\n",
    "\n",
    "This means that ***If we don't have enough training set (which could decrease the rank of A), we still can't find the closed solution.***\n",
    "\n",
    "Besides this, ***the computational cost for matrix calculation is very high so it is definitely not a good idea to find the closed solution for a large amount of data.***\n",
    "\n",
    "To tackle those problems, we could use gradient descent based method instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Gradient Descent Based Method\n",
    "We could use gradient descent based method instead as it could save a lot of time while keeping a relatively high accuracy.\n",
    "\n",
    "The gradient descent based method contains the following main steps:\n",
    "\n",
    "1. Randomly choose t points from the total N points as the training set and the rest could form the validation set.\n",
    "\n",
    "2. Calculate the training loss using the current model and training set.\n",
    "\n",
    "3. Update the model according to the current gradient information. \n",
    "\n",
    "4. Calculate the validation loss using the updated model and validation set.\n",
    "\n",
    "5. Repeat step 1 to 4 until reaching the maximum number of epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.7.1 Detailed Gradient Descent Method: Stochastic Gradient Descent (SGD)\n",
    "The general equation to optimize the model is: $\\theta=\\theta - rate\\cdot\\nabla _\\theta R(\\theta)$\n",
    "\n",
    "Theoretically, we need to use the entire dataset to compute gradient accurately and optimize the model. However, this could be painful if we have lots of data.\n",
    "\n",
    "Alternatively, we could estimate the gradient by calculating the accurate gradient at a specific chosen point, which is known as stochastic gradient descent (SGD).\n",
    "\n",
    "This method could significantly decrease the computational cost and it makes lots of machine learning and deep learning algrotihms powerful.\n",
    "\n",
    "Our objective function (empirical risk) is: $R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{2}(y_i-\\theta \\cdot x_i)^2$\n",
    "\n",
    "Taking the first derivative with respect to $\\theta$: $\\nabla _\\theta R(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N} (y_i-\\theta \\cdot x_i)x_i = \\frac{1}{N}\\sum_{i=1}^{N} (\\theta \\cdot x_i-y_i)x_i$\n",
    "\n",
    "Taking the learning rate into account: $\\theta _{new}=\\theta -rate\\cdot\\frac{1}{N}\\sum_{i=1}^{N} (\\theta \\cdot x_i-y_i)x_i$\n",
    "\n",
    "Because we are only interested in one specfic point, we could simplify it into: $\\theta _{new}=\\theta -rate\\cdot(\\theta \\cdot x_i-y_i)x_i$ where $i$ is randomly chosen from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Regularisation\n",
    "Regularisation could be considered as a resistance for model to perfectly fit the training data.\n",
    "\n",
    "If our model could perfectly fit the training data (very low training loss), this means that it could also fit the noise within the training set very well and generally can't predict the result for unseen data.\n",
    "\n",
    "By adding the regularisation term, only a very big change could update the model, which makes it more robust and stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.1 Gradient Descent Based Method with Regularisation (L2)\n",
    "Within this tutorial, we will focus on the L2 Regularisation.\n",
    "\n",
    "The new objective function becomes: $J_ {\\lambda, N}(\\theta)=\\frac{\\lambda}{2} \\lVert \\theta \\rVert_2^2 + R(\\theta)=\\frac{\\lambda}{2}\\sum_{i=0}^N \\theta _i ^2 + R(\\theta)$\n",
    "\n",
    "Similarly, by taking the first derivative with respect to $\\theta$ and only interested in one point, we could get: $\\theta _{new}=(1-rate\\cdot\\lambda)\\theta -rate\\cdot(\\theta\\cdot x_i-y_i)x_i$ where $i$ is randomly chosen from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Performance Evaluation\n",
    "There are some common used evaluation metrics we could use to evaluate the performance of our linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.9.1 Mean Square Error (MSE) and Root Mean Square Error (RMSE)\n",
    "MSE and RMSE could directly compute the difference between the prediction and the truth value. \n",
    "\n",
    "RMSE is just simply the root of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.9.2 Coefficient of Determination ($R^2$)\n",
    "$R^2$ the proportion of the variation in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "It could measure how well the predictions could approximate hte real data points.\n",
    "\n",
    "The more the score is close to 1, the better performance our linear regression model have.\n",
    "\n",
    "This is calculated by: $R^2=1-\\frac{SS_{residual}}{SS_{total}}$ where $SS_{residual}=\\sum_{i} (y_i-\\theta x_i)^2$ and $SS_{total}=\\sum_{i} (y_i-\\bar{y})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear Regression from Scratch\n",
    "For this tutorial, we are going to solve the Boston House Price Estimation problem step by step.\n",
    "\n",
    "We will directly start implementing the multiple linear regression algorithm as it could be also applied to linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Dataset Loading\n",
    "The whole dataset contains 506 houses with 13 different features:\n",
    "\n",
    "1. CRIM: the per capita crime rate of the town\n",
    "\n",
    "2. ZN: the proportion of residential land zoned for lots over 25,000 sq.ft\n",
    "\n",
    "3. INDUS: the proportion of non-retail nusiness acres per town\n",
    "\n",
    "4. CHAS: Charles River dummy variable (1 if tract bounds river, 0 otherwise)\n",
    "\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "6. RM: average number of rooms per dwelling\n",
    "\n",
    "7. AGE: the proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "8. DIS: weighted distances to five Boston emplyment centers\n",
    "\n",
    "9. RAD: index of accessibility to radial highways\n",
    "\n",
    "10. TAX: full-value property-tax rate per $10,000\n",
    "\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "\n",
    "12. B: 1000(Bk — 0.63)², where Bk is the proportion of [people of African American descent] by town\n",
    "\n",
    "13. Percentage of lower status of the population\n",
    "\n",
    "Every feature has its own scale, which will significantly effect the performance of the model. It is very common trick to normalise each feature into 0~1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "def loadBoston():\n",
    "    '''\n",
    "        Load the boston house price dataset and return points\n",
    "    and labels separately\n",
    "\n",
    "    Return:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "    '''\n",
    "    # load_boston() could direcly load the dataset into the required form\n",
    "    # For more information, please check: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "    dataset, label=load_boston(return_X_y=True)\n",
    "\n",
    "    return dataset, label\n",
    "\n",
    "dataset, label=loadBoston()\n",
    "print(f\"There are {dataset.shape[0]} houses within the dataset and each house has {dataset.shape[1]} features.\")\n",
    "print(f\"There are also {label.shape[0]} corresponding prices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we need to insert a column of \"1\" at the beginning of dataset to change the shape into (N, d+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def addColumn(dataset):\n",
    "    '''\n",
    "        Add a column of \"1\" to the beginning of the dataset\n",
    "    to change the shape into (N, d+1)\n",
    "\n",
    "    Argument:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, dataset with an extra column, (the number of houses, the number of features for each house+1)\n",
    "    '''\n",
    "    # np.ones() could be used to construct an array full of 1 with the given shape\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.ones.html\n",
    "    column=np.ones((1, dataset.shape[0]))\n",
    "    \n",
    "    # np.insert() could be used to insert the column at the specific position\n",
    "    # obj=0 means we want to insert at the index 0\n",
    "    # axis=1 means we want to insert a column\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.insert.html?highlight=insert#numpy.insert\n",
    "    return np.insert(dataset, 0, column, axis=1)\n",
    "\n",
    "dataset, label=loadBoston()\n",
    "modifiedDataset=addColumn(dataset)\n",
    "print(f\"After modification, there are {modifiedDataset.shape[0]} houses now and each house has {modifiedDataset.shape[1]} features.\")\n",
    "print(f\"The first feature value of the house is: {modifiedDataset[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 $\\theta$  Initialisation\n",
    "For the gradient descent based method, we need to have an initial $\\theta$ matrix, which is full of zero with shape (d+1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseTheta(dataset):\n",
    "    '''\n",
    "        Initialise and return the theta matrix according to the shape of the dataset\n",
    "    \n",
    "    Argument:\n",
    "        dataset: np.ndarray, dataset with an extra column, (the number of houses, the number of features for each house+1)\n",
    "    \n",
    "    Return:\n",
    "        theta: np.ndarray, full of 0, (the number of features for each house+1,1)\n",
    "    '''\n",
    "    # np.zeros could be used to construct an array full of 0 with the given shape\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.zeros.html?highlight=zeros#numpy.zeros\n",
    "    return np.zeros((dataset.shape[1], 1))\n",
    "\n",
    "dataset, label=loadBoston()\n",
    "modifiedDataset=addColumn(dataset)\n",
    "theta=initialiseTheta(modifiedDataset)\n",
    "print(f\"The shape of the initial theta is: {theta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Closed Solution (No Regularisation)\n",
    "As we mentioned in the section 3.6.3, our solution could be directly found by $\\theta=(X^TX)^{-1}X^TY$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "def closedSolution(dataset, label):\n",
    "    '''\n",
    "        Return the optimized linear regression model by \n",
    "    substituting into the closed form solution\n",
    "\n",
    "        Report when the inverse matrix can't be found\n",
    "\n",
    "    Argument:\n",
    "        dataset: np.ndarray, dataset with an extra column, (the number of houses, the number of features for each house+1)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "\n",
    "    Return:\n",
    "        result: np.ndarray, the optimized theta for the dataset, (the number of features for each house+1,1)\n",
    "    '''\n",
    "    try:\n",
    "        # np.linalg.inv() could find the inverse matrix of the given input matrix\n",
    "        # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html\n",
    "\n",
    "        # np.matmul() could calculate the normal matrix multiplication\n",
    "        # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\n",
    "\n",
    "        # np.transpose() could find the transpose matrix of the input matrix\n",
    "        # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\n",
    "        firstPart=np.linalg.inv(np.matmul(np.transpose(dataset), dataset))\n",
    "        secondPart=np.matmul(np.transpose(dataset), label)\n",
    "        return np.matmul(firstPart, secondPart)\n",
    "    except LinAlgError:\n",
    "        print(\"Can't find the closed solution due to invalid matrix inversion.\")\n",
    "\n",
    "# np.random.rand() could be used to generate a random array with given shape\n",
    "# For more information, please check: https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n",
    "\n",
    "# Here we test N=3, d+1=4 and we expect a (4,1) result\n",
    "testX=np.random.rand(3,4)\n",
    "testLabel=np.random.rand(3,1)\n",
    "print(f\"The shape of the result is: {closedSolution(testX, testLabel).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Gradient Descent Based Method (No Regularisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.1 Split Training, Validation and Test Set\n",
    "As mentioned in the section 3.7, the training set is randomly selected and the rest is chosen as validation set.\n",
    "\n",
    "Normally, we could divide the whole dataset into 70% as training set, 20% as validation set and 10% as test set.\n",
    "\n",
    "This means that both training and validation sets are different for each epoch but the test set is unchanged.\n",
    "\n",
    "Other different proportions could also work but we need to make sure we do have enough training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def splitDataset(dataset, label, trainProportion, validationProportion):\n",
    "    '''\n",
    "        Split and return the training, validation and test dataset and corresponding\n",
    "    labels as a dictionary according to the given proportion\n",
    "\n",
    "        The test set is always the last proportion of dataset and the rest\n",
    "    are used for choosing training and validation sets\n",
    "\n",
    "        The training and validation sets are randomly chosen every time\n",
    "    \n",
    "    Argument:\n",
    "        dataset: np.ndarray, dataset with an extra column, (the number of houses, the number of features for each house+1)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "        trainProportion: float, the proportion of training set, from 0 to 1\n",
    "        validationProportion: float the proportion of validation set, from 0 to 1\n",
    "\n",
    "    Return:\n",
    "        result: dictionary, contains the training/validation/test points and corresonding labels as key-value pairs\n",
    "    '''\n",
    "    result={}\n",
    "\n",
    "    # Find the number of points within training, validation and test sets\n",
    "    trainNumber=int(dataset.shape[0]*trainProportion)\n",
    "    validationNumber=int(dataset.shape[0]*validationProportion)\n",
    "    testNumber=dataset.shape[0]-trainNumber-validationNumber\n",
    "\n",
    "    # Initilise a list containing all available indexes in the rest part\n",
    "    restIndex=[i for i in range(trainNumber+validationNumber)]\n",
    "    \n",
    "    # We want randomly choose a number of indexes to construct the training set\n",
    "    # random.sample() could randomly choose the given number of elements from the list without replacement\n",
    "    # For more information, please check: https://docs.python.org/3/library/random.html\n",
    "    trainIndex=random.sample(restIndex, trainNumber)\n",
    "\n",
    "    # The indexes within the validation set is the left part of restIndex which is not in the training set\n",
    "    validationIndex=[index for index in restIndex if index not in trainIndex]\n",
    "\n",
    "    # Add the right points and the corresonding labels to the result\n",
    "    result[\"trainPoint\"]=dataset[trainIndex]\n",
    "    result[\"trainLabel\"]=label[trainIndex]\n",
    "\n",
    "    result[\"validationPoint\"]=dataset[validationIndex]\n",
    "    result[\"validationLabel\"]=label[validationIndex]\n",
    "\n",
    "    result[\"testPoint\"]=dataset[-testNumber:]\n",
    "    result[\"testLabel\"]=label[-testNumber:]\n",
    "\n",
    "    return result\n",
    "\n",
    "dataset, label=loadBoston()\n",
    "modifiedDataset=addColumn(dataset)\n",
    "result=splitDataset(modifiedDataset, label, 0.7, 0.2)\n",
    "\n",
    "trainPoint, trainLabel=result[\"trainPoint\"], result[\"trainLabel\"]\n",
    "validationPoint, validationLabel=result[\"validationPoint\"], result[\"validationLabel\"]\n",
    "testPoint, testLabel=result[\"testPoint\"], result[\"testLabel\"]\n",
    "\n",
    "print(f\"There are {trainPoint.shape[0]} houses in the training set and each house has {trainPoint.shape[1]} features.\")\n",
    "print(f\"There are {trainLabel.shape[0]} corresponding prices in the training set.\")\n",
    "print(\" \")\n",
    "print(f\"There are {validationPoint.shape[0]} houses in the validation set and each house has {validationPoint.shape[1]} features.\")\n",
    "print(f\"There are {validationLabel.shape[0]} corresponding prices in the validation set.\")\n",
    "print(\" \")\n",
    "print(f\"There are {testPoint.shape[0]} houses in the test set and each house has {testPoint.shape[1]} features.\")\n",
    "print(f\"There are {testLabel.shape[0]} corresponding prices in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 Find Prediction Using $\\theta$\n",
    "The prediction could be simply calculate by the matrix multiplication between $\\theta$ and $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, theta):\n",
    "    '''\n",
    "        Return the corresponding result by matrix multiplication\n",
    "    \n",
    "    Argument:\n",
    "        input: np.ndarray, arbitrary number of house information, (arbitrary number, the number of features for each house+1)\n",
    "        theta: np.ndarray, represents the linear relationship within the dataset, (the number of features for each house+1, 1)\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, the predicted prices, (arbitrary number, 1)\n",
    "    '''\n",
    "    # The order of theta and input is very important\n",
    "    return np.matmul(input, theta)\n",
    "\n",
    "testInput=np.random.rand(5,6)\n",
    "testTheta=np.random.rand(6,1)\n",
    "result=predict(testInput, testTheta)\n",
    "print(f\"The shape of the prediction is: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.3 Mean Square Error (MSE)\n",
    "As mentioned in the section 3.6.1, our empirical risk is equivalent to 0.5*MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def MSE(prediction, target):\n",
    "    '''\n",
    "        Return the MSE between the prediction and the target.\n",
    "\n",
    "        This function could be used for multiple 1-feature result.\n",
    "    \n",
    "    Argument:\n",
    "        prediction: np.ndarray, prediction made by the model, (the number of houses, 1)\n",
    "        target: np.ndarray, the corresponding truth value, (the number of houses, 1)\n",
    "    '''\n",
    "    # Find the difference vector between the prediction and the target\n",
    "    difference=prediction-target\n",
    "    \n",
    "    # Square the difference vector\n",
    "    # np.squrare() could square every element within the vector\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.square.html\n",
    "    differenceSquare=np.square(difference)\n",
    "\n",
    "    # Find the mean of the squared difference\n",
    "    # np.mean() could find the mean of elements in the given axis\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "    return np.mean(differenceSquare)\n",
    "\n",
    "prediction=np.array([1,2,3])\n",
    "target=np.array([4,5,6])\n",
    "print(f\"The MSE loss between the prediction and the target is: {MSE(prediction, target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.4 Update the $\\theta$ Using Gradient\n",
    "As mentioned in the section 3.7.1, we could optimise the $\\theta$ by using: $\\theta _{new}=\\theta -rate\\cdot(\\theta \\cdot x_i-y_i)x_i$ where $i$ is randomly chosen from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTheta(x, y, currentTheta, learningRate):\n",
    "    '''\n",
    "        Update and return the new theta according to the optimisation rule\n",
    "    \n",
    "    Argument:\n",
    "        x: np.ndarray, a random point chosen from the training point, (1, the number of features for each house+1)\n",
    "        y: np.ndarray, the corresponding chosen label from the training point, (1,1)\n",
    "        currentTheta: np.ndarray, the current theta before optimisation, (the number of features for each house+1, 1)\n",
    "        learningRate: float, determine the step size along the gradient direction in the gradient descent\n",
    "\n",
    "    Return:\n",
    "        result: np.ndarray, the updated theta, (the number of features for each house+1, 1)\n",
    "    '''\n",
    "    # np.dot() could be used to calculate the dot product of the 2 inputs\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "    \n",
    "    # The order of x and theta here is very important\n",
    "    gradient=(np.dot(x, currentTheta)-y)*x\n",
    "    \n",
    "    # Be careful: the gradient here should has the shape (1, the number of features for each house+1), so we need to use the transpose to update the theta\n",
    "    return currentTheta-learningRate*np.transpose(gradient)\n",
    "\n",
    "testX=np.random.rand(1, 10)\n",
    "testY=np.random.rand(1, 1)\n",
    "testTheta=np.random.rand(10, 1)\n",
    "learningRate=0.1\n",
    "updatedTheta=updateTheta(testX, testY, testTheta, learningRate)\n",
    "print(f\"The original theta has the shape: {testTheta.shape}\")\n",
    "print(f\"The upated theta has the shape: {updatedTheta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.5 Plot Training Loss and Validation Loss\n",
    "We need a function to plot the training and validation loss in the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(trainloss, validationLoss, name):\n",
    "    '''\n",
    "        Plot the training loss and validation loss in the same graph\n",
    "    \n",
    "    Argument:\n",
    "        trainLoss: list, contains the training loss at every epoch\n",
    "        validationLoss: list, contains the validation loss at every epoch\n",
    "        name: string, the name of the plot\n",
    "    '''\n",
    "    plt.plot(trainloss, label=\"Training Loss\")\n",
    "    plt.plot(validationLoss, label=\"Validation Loss\")\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Training Loss/Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.6 Evaluation Metrics (MSE and $R^2$ Score)\n",
    "\n",
    "We will use MSE and $R^2$ score as our evaluation metrics.\n",
    "\n",
    "We have previously implemented the MSE so we could directly use it here.\n",
    "\n",
    "$R^2$ score is calculated by: $R^2=1-\\frac{SS_{residual}}{SS_{total}}$ where $SS_{residual}=\\sum_{i} (y_i-\\theta x_i)^2$ and $SS_{total}=\\sum_{i} (y_i-\\bar{y})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(prediction, target, name):\n",
    "    '''\n",
    "        Evaluate and report the MSE and R2 score\n",
    "    between the prediction and the target\n",
    "\n",
    "        The R2 score could also be simply implemented by using: sklearn.metrics.r2_score()\n",
    "        For more information, please check: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score\n",
    "\n",
    "    Argument:\n",
    "        prediction: np.ndarray, prediction made by the model, (the number of houses, 1)\n",
    "        target: np.ndarray, the corresponding truth value, (the number of houses, 1)\n",
    "        name: string, the name of the model\n",
    "    '''\n",
    "    # Find the MSE between the prediction and target\n",
    "    mse=MSE(prediction, target)\n",
    "\n",
    "    # Find the SS residual, which is simply MSE*the number of houses\n",
    "    ssResidual=mse*prediction.shape[0]\n",
    "\n",
    "    # Find the SS total\n",
    "    # np.full() could be used to construct an array full of constant value with the given shape\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.full.html\n",
    "    \n",
    "    # 1. Construct an array which is full of the mean of target\n",
    "    mean=np.full(target.shape, np.mean(target))\n",
    "    # 2. Find the difference array\n",
    "    difference=target-mean\n",
    "    # 3. Find the square of the differnce array in element-wise\n",
    "    differenceSquare=np.square(difference)\n",
    "    # 4. Find the sum\n",
    "    ssTotal=np.sum(differenceSquare)\n",
    "    # 5. Find the R2 score\n",
    "    r2=1-ssResidual/ssTotal\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"The {name} method got MSE: {mse}\")\n",
    "    print(f\"The {name} method got R2 score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.6 Final Implementation\n",
    "As mentioned in the section 3.7, the algorithm contains the following main steps:\n",
    "\n",
    "1. Randomly choose t points from the total N points as the training set and the rest could form the validation set.\n",
    "\n",
    "2. Calculate the training loss using the current model and training set.\n",
    "\n",
    "3. Update the model according to the current gradient information. \n",
    "\n",
    "4. Calculate the validation loss using the updated model and validation set.\n",
    "\n",
    "5. Repeat step 1 to 4 until reaching the maximum number of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def linearRegression(dataset, label, epoch, learningRate, trainProportion, validationProportion, plot=True):\n",
    "    '''\n",
    "        Run the multiple linear regression algorithm using the gradient descent method\n",
    "    \n",
    "    Argument:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "        epoch: int, the maximum number of repeats to run\n",
    "        learningRate: float, determine the step size along the gradient direction in the gradient descent\n",
    "        trainProportion: float, the proportion of training set, from 0 to 1\n",
    "        validationProportion: float the proportion of validation set, from 0 to 1\n",
    "        plot: boolean, True if plotting the training and validation loss\n",
    "\n",
    "    Return:\n",
    "        theta: np.ndarray, represents the linear relationship within the dataset, (the number of features for each house+1, 1)\n",
    "    '''\n",
    "    # Initialise the training loss and validation loss\n",
    "    trainLoss, validationLoss=[], []\n",
    "    # Initialise the theta (full of 0)\n",
    "    theta=initialiseTheta(dataset)\n",
    "\n",
    "    # Repeat the calculation until reaching the maximum number of epoch\n",
    "    for i in range(epoch):\n",
    "        # Randomly construct the training and validation sets\n",
    "        data=splitDataset(dataset, label, trainProportion, validationProportion)\n",
    "\n",
    "        # Find the prediction for the training set using the current theta\n",
    "        trainPrediction=predict(data[\"trainPoint\"], theta)\n",
    "\n",
    "        # Calculate and store the training loss for this epoch\n",
    "        trainLossEpoch=0.5*MSE(trainPrediction, data[\"trainLabel\"])\n",
    "        trainLoss.append(trainLossEpoch)\n",
    "\n",
    "        # Update the theta according to the gradient\n",
    "        # Be careful: As we are using SGD, only one point and one label is required for gradient calculation\n",
    "        # We could always use the first point as the training set is randomly formed in every epoch\n",
    "        # We also need to use transpose vector to make.shape the shape into (1, the number of features for each house+1)\n",
    "        theta=updateTheta(np.array([data[\"trainPoint\"][0]]), data[\"trainLabel\"][0], theta, learningRate)\n",
    "        \n",
    "        # Find the prediction for the validation loss for this epoch\n",
    "        validationPrediction=predict(data[\"validationPoint\"], theta)\n",
    "\n",
    "        # Calculate and store the validation loss for this epoch\n",
    "        validationLossEpoch=0.5*MSE(validationPrediction, data[\"validationPoint\"])\n",
    "        validationLoss.append(validationLossEpoch)\n",
    "    \n",
    "    # Finish training here\n",
    "    # Plot the training and validation loss if required\n",
    "    if plot:\n",
    "        plotLoss(trainLoss, validationLoss, \"Linear Regression from Scartch with No Regularisation\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    testPrediction=predict(data[\"testPoint\"], theta)\n",
    "    evaluate(testPrediction, data[\"testLabel\"], \"linear regression from scratch\")\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Gradient Descent Based Method (Ridge Regression)\n",
    "To add this L2 regularisation into account, we could just slightly modify the updateTheta() function.\n",
    "\n",
    "After adding the L2 regularisation, we could use $\\theta _{new}=(1-rate\\cdot\\lambda)\\theta -rate\\cdot(\\theta\\cdot x_i-y_i)x_i$ to update the theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def updateThetaRidge(x, y, currentTheta, learningRate, regularisationCoefficient):\n",
    "    '''\n",
    "        Update and return the new theta according to the optimisation\n",
    "    rule with L2 regularisation\n",
    "    \n",
    "    Argument:\n",
    "        x: np.ndarray, a random point chosen from the training point, (1, the number of features for each house+1)\n",
    "        y: np.ndarray, the corresponding chosen label from the training point, (1,1)\n",
    "        currentTheta: np.ndarray, the current theta before optimisation, (the number of features for each house+1, 1)\n",
    "        learningRate: float, determine the step size along the gradient direction in the gradient descent\n",
    "        regularisationCoefficient: float, determine how strong the regularisation could effect the optimisation\n",
    "\n",
    "    Return:\n",
    "        result: np.ndarray, the updated theta, (the number of features for each house+1, 1)\n",
    "    '''  \n",
    "    # The order of x and theta here is very important\n",
    "    gradient=(np.dot(x, currentTheta)-y)*x\n",
    "    \n",
    "    # Be careful: the gradient here should has the shape (1, the number of features for each house+1), so we need to use the transpose to update the theta\n",
    "    return (1-regularisationCoefficient*learningRate)*currentTheta-learningRate*np.transpose(gradient)\n",
    "\n",
    "def Ridge(dataset, label, epoch, learningRate, regularisationCoefficient, trainProportion, validationProportion, plot=True):\n",
    "    '''\n",
    "        Run the multiple linear regression algorithm with L2 regularisation\n",
    "    using the gradient descent method\n",
    "    \n",
    "    Argument:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "        epoch: int, the maximum number of repeats to run\n",
    "        learningRate: float, determine the step size along the gradient direction in the gradient descent\n",
    "        regularisationCoefficient: float, determine how strong the regularisation could effect the optimisation\n",
    "        trainProportion: float, the proportion of training set, from 0 to 1\n",
    "        validationProportion: float the proportion of validation set, from 0 to 1\n",
    "        plot: boolean, True if plotting the training and validation loss\n",
    "\n",
    "    Return:\n",
    "        theta: np.ndarray, represents the linear relationship within the dataset, (the number of features for each house+1, 1)\n",
    "    '''\n",
    "    # Initialise the training loss and validation loss\n",
    "    trainLoss, validationLoss=[], []\n",
    "    # Initialise the theta (full of 0)\n",
    "    theta=initialiseTheta(dataset)\n",
    "\n",
    "    # Repeat the calculation until reaching the maximum number of epoch\n",
    "    for i in range(epoch):\n",
    "        # Randomly construct the training and validation sets\n",
    "        data=splitDataset(dataset, label, trainProportion, validationProportion)\n",
    "\n",
    "        # Find the prediction for the training set using the current theta\n",
    "        trainPrediction=predict(data[\"trainPoint\"], theta)\n",
    "\n",
    "        # Calculate and store the training loss for this epoch\n",
    "        trainLossEpoch=0.5*MSE(trainPrediction, data[\"trainLabel\"])\n",
    "        trainLoss.append(trainLossEpoch)\n",
    "\n",
    "        # Update the theta according to the gradient with L2 regularisation\n",
    "        # Be careful: As we are using SGD, only one point and one label is required for gradient calculation\n",
    "        # We could always use the first point as the training set is randomly formed in every epoch\n",
    "        # We also need to use transpose vector to make.shape the shape into (1, the number of features for each house+1)\n",
    "        theta=updateThetaRidge(np.array([data[\"trainPoint\"][0]]), data[\"trainLabel\"][0], theta, learningRate, regularisationCoefficient)\n",
    "        \n",
    "        # Find the prediction for the validation loss for this epoch\n",
    "        validationPrediction=predict(data[\"validationPoint\"], theta)\n",
    "\n",
    "        # Calculate and store the validation loss for this epoch\n",
    "        validationLossEpoch=0.5*MSE(validationPrediction, data[\"validationPoint\"])\n",
    "        validationLoss.append(validationLossEpoch)\n",
    "    \n",
    "    # Finish training here\n",
    "    # Plot the training and validation loss if required\n",
    "    if plot:\n",
    "        plotLoss(trainLoss, validationLoss, \"ridge regression from scartch\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    testPrediction=predict(data[\"testPoint\"], theta)\n",
    "    evaluate(testPrediction, data[\"testLabel\"], \"ridge regression from scratch\")\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Linear Regression with No Regularisation Using Sklearn\n",
    "Scikit-learn is a popular API for machine learning in Python. We could easily build the linear regression model using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linearRegressionSklearn(dataset, label, trainProportion, validationProportion):\n",
    "    '''\n",
    "        Run the multiple linear regression algorithm using scikit-learn\n",
    "\n",
    "        We train the same amount of data and use the same test set for fair\n",
    "    comparison\n",
    "\n",
    "        For more information, please check: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "    Argument:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "        trainProportion: float, the proportion of training set, from 0 to 1\n",
    "        validationProportion: float the proportion of validation set, from 0 to 1\n",
    "\n",
    "    Return:\n",
    "        model: sklearn.linear_model._base.LinearRegression, the optimised model\n",
    "    '''\n",
    "    data=splitDataset(dataset, label, trainProportion, validationProportion)\n",
    "\n",
    "    # Initialise a linear regression model using sklearn\n",
    "    model=LinearRegression()\n",
    "    # Fit the model using the training set\n",
    "    model.fit(dataset[\"trainPoint\"], dataset[\"trainLabel\"])\n",
    "\n",
    "    # Get prediction and evaluate the model\n",
    "    testPrediction=model.predict(dataset[\"testPoint\"])\n",
    "    evaluate(testPrediction, data[\"testLabel\"], \"linear regression using sklearn\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ridge Regression Using Sklearn\n",
    "We could also simply build the ridge regression model using scikit-learn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridgeRegressionSklearn(dataset, label, regularisationCoefficient, trainProportion, validationProportion):\n",
    "    '''\n",
    "        Run the ridge regression algorithm using scikit-learn\n",
    "\n",
    "        We train the same amount of data and use the same test set for fair\n",
    "    comparison\n",
    "\n",
    "        For more information, please check: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n",
    "\n",
    "    Argument:\n",
    "        dataset: np.ndarray, all houses with 13 features, (the number of houses, the number of features for each house)\n",
    "        label: np.ndarray, all corresponding prices, ideally (the number of houses, 1)\n",
    "        regularisationCoefficient: float, determine how strong the regularisation could effect the optimisation\n",
    "        trainProportion: float, the proportion of training set, from 0 to 1\n",
    "        validationProportion: float the proportion of validation set, from 0 to 1\n",
    "\n",
    "    Return:\n",
    "        model: sklearn.linear_model._base.LinearRegression, the optimised model\n",
    "    '''\n",
    "    data=splitDataset(dataset, label, trainProportion, validationProportion)\n",
    "\n",
    "    # Initialise a ridge regression model using sklearn\n",
    "    model=Ridge(alpha=regularisationCoefficient)\n",
    "    # Fit the model using the training set\n",
    "    model.fit(dataset[\"trainPoint\"], dataset[\"trainLabel\"])\n",
    "\n",
    "    # Get prediction and evaluate the model\n",
    "    testPrediction=model.predict(dataset[\"testPoint\"])\n",
    "    evaluate(testPrediction, data[\"testLabel\"], \"ridge regression using sklearn\")\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
